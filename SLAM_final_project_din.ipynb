{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_e_cQTYb-Zj"
   },
   "source": [
    "# ROB5002-41 *Introduction to SLAM*\n",
    "\n",
    " **Final Project**\n",
    "\n",
    "Victoria Park Dataset (for more information such as vehicle modeling, dynamic model and measurement model see [link](https://www-personal.acfr.usyd.edu.au/nebot/victoria_park.htm).)\n",
    "\n",
    "<img src = \"https://www.researchgate.net/publication/225106837/figure/fig7/AS:668616529805327@1536422011812/FastSLAM-20-applied-to-the-Victoria-Park-Dataset.png\" height = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1qKjqR0d1lB"
   },
   "source": [
    "**DATASET** - *arranged in time-order*\n",
    "\n",
    "Example of the dataset (note that gps is gt, it is not used for EFK but used only for evaluation):\n",
    "\n",
    "     | data[0] =  ('gps', [time, zx_gt, zy_gt, ztheta_gt])\n",
    "     | data[1] =  ('odometry', [time, ve, alpha])\n",
    "     | data[2] =  ('odometry', [time, ve, alpha])\n",
    "     | data[3] =  ('measurements', [time, d0, phi0, d1, phi1, ...])\n",
    "     | data[4] =  ('odometry', [time, ve, alpha])\n",
    "     | data[5] =  ('odometry', [time, ve, alpha])\n",
    "     | data[6] =  ('gps', [time, zx_gt, zy_gt, ztheta_gt])\n",
    "     | data[7] =  ('odometry', [time, ve, alpha])\n",
    "     | data[8] =  ('odometry', [time, ve, alpha])\n",
    "     | data[9] =  ('measurements', [time, d0, phi0, d1, phi1, ...])\n",
    "     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRdUi1GKhexy"
   },
   "source": [
    "# TODO\n",
    "\n",
    "**1. data.npy**\n",
    "\n",
    "*   Assume that all landmarks are static\n",
    "*   implement EKF SLAM\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?id=1ELl2QZ3D_-_LSzbHLD6l_Qs5LJ10i_jI\" height = 500>\n",
    "\n",
    "\n",
    "\n",
    "\\\n",
    "\n",
    "\n",
    "**2. data_dynamic.npy**\n",
    "\n",
    "*   Assume that all the trees are going crazy, uprooting themselves and moving at a velocity of [0.05, 0.05].\n",
    "*   Trees are starting to move from the beginning of the data (starts to move right after data[0])\n",
    "*   implement EKF SLAM with consideration of these dynamic landmarks.\n",
    "\n",
    "\\\n",
    "\n",
    "* **EX) with common EKF SLAM (left), with EKF SLAM for dynamic landmarks (right)**\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?id=1E_EzqCIchZcV1O6YSE5rU_5LHXOIpbEA\" width = 500>\n",
    "<img src = \"https://drive.google.com/uc?id=1EL75ntOpKoOvzH88XnT5yt-j-KFrrMQ5\" width = 500>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm1i9_BFndoL"
   },
   "source": [
    "**Things to Submit**\n",
    "\n",
    " - your code (based on this jupyter notebook)\n",
    "\n",
    " - Report (minimum requirement)\n",
    "  - Include 'introduction', 'method', 'experiments' and 'conclusion'\n",
    "  - Explain your code and the reasons behind your implementation choices.\n",
    "  - Consider two cases: data.npy and data_dynamic.npy. Explain your strategy and report the results such as ATE and num_of_seen_landmarks. Explain your results.\n",
    "  - Explain Q and R. What are these? Discuss the effect of Q and R.\n",
    "  - Explain the limitation of SLAM in the aspect of the dynamic environment. Compare the number of landmarks of each cases. Discuss your results.\n",
    "  - (Bonus) explain the function \"compute_data_association\". What does this function do? Why we need it? Why this function compute below? Discuss the limitation of this function.\n",
    "  \n",
    "            r = np.array(np.array(measurements[k][0:2]) - np.array(z_hat))\n",
    "            S_inv = np.linalg.inv(np.matmul(np.matmul(H,SIGMA),np.transpose(H)) + R[:2,:2])\n",
    "            MD = np.matmul(np.matmul(np.transpose(r),S_inv), r)\n",
    "\n",
    "**Recommendation**\n",
    " - No need to implement additional functions or parameters\n",
    " - Refer to see the textbook, *5.1. EXTENDED KALMAN FILTER* and *6.2 EKF SLAM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1749722219368,
     "user": {
      "displayName": "Gabriel García A.",
      "userId": "15710136422793665215"
     },
     "user_tz": -540
    },
    "id": "OJrxihsnqMpi"
   },
   "outputs": [],
   "source": [
    "# codes are started after this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1358,
     "status": "ok",
     "timestamp": 1749722221698,
     "user": {
      "displayName": "Gabriel García A.",
      "userId": "15710136422793665215"
     },
     "user_tz": -540
    },
    "id": "Tz1r_ewsHUYV",
    "outputId": "b16f2e89-6b85-4947-a992-10a0d6ec675e"
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1749722223855,
     "user": {
      "displayName": "Gabriel García A.",
      "userId": "15710136422793665215"
     },
     "user_tz": -540
    },
    "id": "DDADBD7VHiB2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats.distributions import chi2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# declare your path\n",
    "#your_project_path = os.path.join('/content/drive/MyDrive/2025_slam/2025slam_finalproj')\n",
    "\n",
    "#os.chdir(your_project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1749722225417,
     "user": {
      "displayName": "Gabriel García A.",
      "userId": "15710136422793665215"
     },
     "user_tz": -540
    },
    "id": "eD6jiY80Jge_"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  DO NOT MODIFY THIS BLOCK\n",
    "'''\n",
    "def solve_cost_matrix_heuristic(M):\n",
    "    n_msmts = M.shape[0]\n",
    "    result = []\n",
    "\n",
    "    ordering = np.argsort(M.min(axis=1))\n",
    "\n",
    "    for msmt in ordering:\n",
    "        match = np.argmin(M[msmt,:])\n",
    "        M[:, match] = 1e8\n",
    "        result.append((msmt, match))\n",
    "\n",
    "    return result\n",
    "\n",
    "def compute_data_association(MU, SIGMA, num_of_seen_landmarks, measurements, R):\n",
    "    def measurement_model(MU, landmark_id):\n",
    "        z_x, z_y, z_theta = MU[:robot_dim]\n",
    "        z_theta = np.arctan2(np.sin(z_theta), np.cos(z_theta))\n",
    "        m_x, m_y = MU[robot_dim + landmark_dim*landmark_id : robot_dim + landmark_dim*(landmark_id+1)]\n",
    "        d = np.sqrt((m_x-z_x)**2 + (m_y-z_y)**2)\n",
    "        phi = np.arctan2((m_y-z_y),(m_x-z_x)) - z_theta\n",
    "        phi = np.arctan2(np.sin(phi), np.cos(phi))\n",
    "        zhat = np.array([d, phi])\n",
    "\n",
    "        H = np.zeros((2,MU.size))\n",
    "        H[0,0] = -(m_x - z_x)/np.sqrt((m_x - z_x)**2+(m_y - z_y)**2)\n",
    "        H[1,0] = (m_y - z_y)/((m_x - z_x)**2+(m_y - z_y)**2)\n",
    "        H[0,1] = -(m_y - z_y)/np.sqrt((m_x - z_x)**2+(m_y - z_y)**2)\n",
    "        H[1,1] = -(m_x - z_x)/((m_x - z_x)**2+(m_y - z_y)**2)\n",
    "        H[0,2] = 0\n",
    "        H[1,2] = -1\n",
    "        H[0,1+2*(landmark_id+1)] = (m_x - z_x)/np.sqrt((m_x - z_x)**2+(m_y - z_y)**2)\n",
    "        H[1,1+2*(landmark_id+1)] = -(m_y - z_y)/((m_x-z_x)**2+(m_y-z_y)**2)\n",
    "        H[0,2+2*(landmark_id+1)] = (m_y - z_y)/np.sqrt((m_x - z_x)**2+(m_y - z_y)**2)\n",
    "        H[1,2+2*(landmark_id+1)] = (m_x-z_x)/((m_x-z_x)**2+(m_y-z_y)**2)\n",
    "\n",
    "        return zhat, H\n",
    "\n",
    "    if num_of_seen_landmarks == 0:\n",
    "        # set association to init new landmarks for all measurements\n",
    "        return np.array([-1 for m in measurements])\n",
    "\n",
    "    A = np.full((len(measurements),len(measurements)),chi2.ppf(0.96, df=2))\n",
    "    cost_mat = np.full((len(measurements), num_of_seen_landmarks), chi2.ppf(0.96, df=2))\n",
    "\n",
    "    for k in range(0,len(measurements)):\n",
    "        for j in range(0,num_of_seen_landmarks):\n",
    "            z_hat,H = measurement_model(MU, j)\n",
    "            # print(measurements[k][0:2])\n",
    "            r = np.array(np.array(measurements[k][0:2]) - np.array(z_hat))\n",
    "            S_inv = np.linalg.inv(np.matmul(np.matmul(H,SIGMA),np.transpose(H)) + R[:2,:2])\n",
    "            MD = np.matmul(np.matmul(np.transpose(r),S_inv), r)\n",
    "            cost_mat[k,j] = MD\n",
    "\n",
    "    cost_mat_conc = np.concatenate((cost_mat, A), axis=1)\n",
    "    temp1 = np.copy(cost_mat)\n",
    "    results = solve_cost_matrix_heuristic(temp1)\n",
    "\n",
    "    assoc = np.zeros(len(measurements),dtype = np.int32)\n",
    "    for k in range(0, len(results)):\n",
    "        # print(cost_mat[results[k][0],results[k][1]])\n",
    "        if cost_mat_conc[results[k][0],results[k][1]] > chi2.ppf(0.99, df=2):\n",
    "            assoc[results[k][0]] = -1\n",
    "        elif cost_mat_conc[results[k][0],results[k][1]] >= chi2.ppf(0.95, df=2):\n",
    "            assoc[results[k][0]] = -2\n",
    "        else:\n",
    "            assoc[results[k][0]] = results[k][1]\n",
    "\n",
    "    return assoc\n",
    "\n",
    "def cal_ate_rmse(state_history, gps_history):\n",
    "    state_history = np.array(state_history)\n",
    "    state_history_t = state_history[:,0]\n",
    "\n",
    "    max_dt = 0.015 # [s]    0.015\n",
    "    ATE_errors = []\n",
    "    # associated idxs\n",
    "    idxs_gps = []\n",
    "    idxs_est = []\n",
    "\n",
    "    for i, gps in enumerate(gps_history):\n",
    "        gps_t = gps[0]\n",
    "        gps_xy = gps[1:3]\n",
    "\n",
    "        # associate\n",
    "        associated_idx = np.argmin(np.abs(state_history_t - gps_t))\n",
    "\n",
    "        if np.abs(state_history_t[associated_idx] - gps_t) < max_dt:\n",
    "            est_xy = state_history[associated_idx][1:3]\n",
    "            ATE_error = (gps_xy[0] - est_xy[0])**2 + (gps_xy[1] - est_xy[1])**2\n",
    "            ATE_errors.append(ATE_error)\n",
    "            idxs_est.append(associated_idx)\n",
    "            idxs_gps.append(i)\n",
    "\n",
    "    ATE_errors = np.array(ATE_errors)\n",
    "    ATE_RMSE = np.sqrt(np.mean(ATE_errors))\n",
    "    print(f\"ATE RMSE: {ATE_RMSE:.3f}[m]\")\n",
    "    return ATE_RMSE, idxs_gps, idxs_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1749722227678,
     "user": {
      "displayName": "Gabriel García A.",
      "userId": "15710136422793665215"
     },
     "user_tz": -540
    },
    "id": "JpRcUfcGjmdg"
   },
   "outputs": [],
   "source": [
    "''' Do not modify these parameters '''\n",
    "# parameters\n",
    "robot_dim = 3\n",
    "landmark_dim = 2\n",
    "num_of_total_landmarks = 500 # assumed to be known\n",
    "\n",
    "vehicle_params = {\n",
    "        \"a\": 3.78,\n",
    "        \"b\": 0.50,\n",
    "        \"L\": 2.83,\n",
    "        \"H\": 0.76\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1749724324942,
     "user": {
      "displayName": "Gabriel García A.",
      "userId": "15710136422793665215"
     },
     "user_tz": -540
    },
    "id": "nx4kUXA4FOAg"
   },
   "outputs": [],
   "source": [
    "def g_dynamic(z, u, dT, vehicle_params):\n",
    "\n",
    "  #print('State before: ', z)\n",
    "  z_new = z.copy()\n",
    "  z_theta = z_new[2]\n",
    "  z_theta = np.arctan2(np.sin(z_theta), np.cos(z_theta))  # normalize angle\n",
    "\n",
    "  a = vehicle_params['a']\n",
    "  b = vehicle_params['b']\n",
    "  L = vehicle_params['L']\n",
    "  H = vehicle_params['H']\n",
    "\n",
    "  ve = u[0]\n",
    "  alpha = u[1]\n",
    "  vc = ve/(1 - np.tan(alpha)*H/L)\n",
    "\n",
    "  dyn_model = np.array([\n",
    "    dT * (vc * np.cos(z_theta) - vc / L * np.tan(alpha) * (a * np.sin(z_theta) + b * np.cos(z_theta))),\n",
    "    dT * (vc * np.sin(z_theta) + vc / L * np.tan(alpha) * (a * np.cos(z_theta) - b * np.sin(z_theta))),\n",
    "    dT * vc / L * np.tan(alpha)\n",
    "    ])\n",
    "\n",
    "  #print('Dynamic model: ', dyn_model)\n",
    "  z_new[:3] += dyn_model\n",
    "  \n",
    "  for i in range(num_of_total_landmarks):\n",
    "    idx = 3 + i * 2\n",
    "    z_new[idx]     += dT * 0.05  # x component\n",
    "    z_new[idx + 1] += dT * 0.05  # y component\n",
    "  \n",
    "  return z_new\n",
    "\n",
    "def g_jacobian(z, u, dT, vehicle_params):\n",
    "\n",
    "  z_theta = z[2]\n",
    "  z_theta = np.arctan2(np.sin(z_theta), np.cos(z_theta))\n",
    "\n",
    "  a = vehicle_params['a']\n",
    "  b = vehicle_params['b']\n",
    "  L = vehicle_params['L']\n",
    "  H = vehicle_params['H']\n",
    "\n",
    "  ve = u[0]\n",
    "  alpha = u[1]\n",
    "  vc = ve/(1 - np.tan(alpha)*H/L)\n",
    "\n",
    "  G13 = -dT*vc*(np.sin(z_theta)+(1/L)*np.tan(alpha)*(a*np.cos(z_theta)-b*np.sin(z_theta)))\n",
    "  G23 = dT*vc*(np.cos(z_theta)-(1/L)*np.tan(alpha)*(a*np.sin(z_theta)+b*np.cos(z_theta)))\n",
    "  g_jacob = np.array([[1, 0, G13], [0, 1, G23],[0, 0, 1]])\n",
    "  return g_jacob\n",
    "\n",
    "def h_measurement(z, landmark_index, h_measurement_out):\n",
    "#def h_measurement(z, landmark_index):\n",
    "  z_x = z[0]\n",
    "  z_y = z[1]\n",
    "  z_theta = z[2]\n",
    "  z_theta = np.arctan2(np.sin(z_theta), np.cos(z_theta))\n",
    "\n",
    "  m_x = z[robot_dim + landmark_dim*landmark_index]\n",
    "  m_y = z[robot_dim + landmark_dim*landmark_index + 1]\n",
    "\n",
    "  del_x = m_x - z_x\n",
    "  del_y = m_y - z_y\n",
    "  r = del_x**2 + del_y**2\n",
    "  sqrt_r = np.sqrt(r)\n",
    "  new_theta = np.arctan2(del_y,del_x)-z_theta\n",
    "  new_theta = np.arctan2(np.sin(new_theta), np.cos(new_theta))\n",
    "  zhat = [[sqrt_r],[new_theta]]\n",
    "\n",
    "  h_measurement_out[0] = zhat[0]\n",
    "  h_measurement_out[1] = zhat[1]\n",
    "\n",
    "  return h_measurement_out\n",
    "\n",
    "#def h_jacobian(z, landmark_index, h_jacob):\n",
    "def h_jacobian(z, landmark_index):\n",
    "\n",
    "  z_x = z[0]\n",
    "  z_y = z[1]\n",
    "\n",
    "  m_x = z[robot_dim + landmark_dim*landmark_index]\n",
    "  m_y = z[robot_dim + landmark_dim*landmark_index + 1]\n",
    "\n",
    "  del_x = m_x - z_x\n",
    "  del_y = m_y - z_y\n",
    "  r = del_x**2 + del_y**2\n",
    "  sqrt_r = np.sqrt(r)\n",
    "\n",
    "  h = np.array([[-sqrt_r*del_x, -sqrt_r*del_y, 0, sqrt_r*del_x, sqrt_r*del_y],[del_y, -del_x, -r, -del_y, del_x]])/r\n",
    "  F_x = np.zeros((5, len(z)))\n",
    "  F_x[:3,:3] = np.eye(3)\n",
    "  F_x[3,3+2*landmark_index]=1\n",
    "  F_x[4,4+2*landmark_index]=1\n",
    "  h_jacob = np.matmul(h,F_x)\n",
    "\n",
    "  return h_jacob\n",
    "\n",
    "# def h_jacobian(z, landmark_index, h_jacob):\n",
    "#   #print(z.shape)\n",
    "#   #print(z)\n",
    "#   print('Landmark index:', landmark_index)\n",
    "#   z_x = z[0]\n",
    "#   z_y = z[1]\n",
    "\n",
    "#   m_x = z[robot_dim + landmark_dim*landmark_index]\n",
    "#   m_y = z[robot_dim + landmark_dim*landmark_index + 1]\n",
    "\n",
    "#   del_x = m_x - z_x\n",
    "#   del_y = m_y - z_y\n",
    "#   delta = np.sqrt(del_x**2 + del_y**2)\n",
    "#   h_jacob = np.array([[-del_x, -del_y, 0, del_x, del_y]/delta,\n",
    "#                       [del_y/delta**2, -del_x/delta**2, -1, -del_y/delta**2, del_x/delta**2],\n",
    "#                       ])\n",
    "#   return h_jacob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1749723696109,
     "user": {
      "displayName": "Gabriel García A.",
      "userId": "15710136422793665215"
     },
     "user_tz": -540
    },
    "id": "8K9KAIFrdEdT",
    "outputId": "a77ce914-04fe-46b8-d324-e82a572bf7dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape:  (1003, 1003)\n",
      "R shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  you can modify these matrices if necessary\n",
    "'''\n",
    "# Dynamic noise\n",
    "Q = np.diag(np.append(np.array([0.05**2, # variance of robot x\n",
    "                                0.05**2, # variance of robot y\n",
    "                   (0.5*np.pi/180)**2]), # variance of robot theta\n",
    "              np.zeros((landmark_dim * num_of_total_landmarks,))))\n",
    "print('Q shape: ', Q.shape)\n",
    "# Measurement noise\n",
    "# R = np.diag([0.5**2,          # variance of landmark r\n",
    "#             (5*np.pi/180)**2] # variance of landmark theta\n",
    "#                     * num_of_total_landmarks)\n",
    "\n",
    "R = np.diag([0.7**3, (7*np.pi/180)**3])\n",
    "print('R shape:', R.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f91b59b78ccf4ba0981a5ad120b9b6f7",
      "ad223f1aecfa40a78e58d138b4c7a208",
      "68c290f602e2494b8163abc65bd6acbf",
      "cdffcd5beaa0446dadfaed054cccdf8d",
      "e496dfe272ae4e0598b22e10bce94e8b",
      "ea26e5ecb058449f9ea183eade40a0df",
      "c63a00a9222d4f09835477110ec0355e",
      "1584c8cd86034a69a7a45aa25ce56ae7",
      "b50b55d8ac934d3e98bc70d61f5b39ce",
      "17b460cfdf22428886fdceeb5d7fb0e1",
      "075ce541f27f4c8fb32529be476545a4"
     ]
    },
    "executionInfo": {
     "elapsed": 44074,
     "status": "ok",
     "timestamp": 1749724375459,
     "user": {
      "displayName": "Gabriel García A.",
      "userId": "15710136422793665215"
     },
     "user_tz": -540
    },
    "id": "MOSqA0VkFOAh",
    "outputId": "23394c7d-8ce7-4c29-ba97-ef2e6369f20d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68ed9e0adb341b7b7f6beb8f0b70e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################################################################\n",
    "# main\n",
    "###########################################################################\n",
    "# Reading dataset\n",
    "# events, robot_init_pose = read_data(len_data=10000, path=your_data_path)\n",
    "\n",
    "events, robot_init_pose = np.load('data.npy', allow_pickle=True)\n",
    "#print('Init robot position:', robot_init_pose)\n",
    "\n",
    "'''\n",
    "  it takes about 10 minuts for all trajectory estimation.\n",
    "  for debuging you can crop the events as the following:\n",
    "'''\n",
    "#events = events[:5000] #\n",
    "\n",
    "time_prev = -1\n",
    "gps_history = []\n",
    "state_history = {\n",
    "    'robot_pose_history': [],\n",
    "    'x': np.array(robot_init_pose),\n",
    "    'P': np.array([.1, .1, 1])\n",
    "}\n",
    "\n",
    "# MU, SIGMA\n",
    "MU = np.append(robot_init_pose, np.zeros((landmark_dim * num_of_total_landmarks, )))\n",
    "SIGMA = np.diag(np.append(1 * np.ones((robot_dim, )),\n",
    "                          100 * np.ones((landmark_dim * num_of_total_landmarks, ))))\n",
    "num_of_seen_landmarks = 0\n",
    "\n",
    "for i, event in tqdm(enumerate(events), total=len(events)):\n",
    "    #print('Event:', event)\n",
    "    time_curr = event[1][0]\n",
    "\n",
    "    if event[0] == 'odometry':\n",
    "        #print('Odometry event:', event[1])\n",
    "        ## For the first data\n",
    "        if time_prev < 0:\n",
    "            time_prev = time_curr\n",
    "            continue\n",
    "        ##\n",
    "\n",
    "        # Dynamic Update\n",
    "        u = event[1][1:]    # u_t = [u_v, u_w]\n",
    "        #print('Odometry event:', u)\n",
    "        dT = time_curr - time_prev\n",
    "        MU = g_dynamic(MU, u, dT, vehicle_params)\n",
    "        #print(\"MU SHAPE\", MU.shape)\n",
    "        g_jacob = g_jacobian(MU, u, dT, vehicle_params)\n",
    "        SIGMA[0:3, 0:3] = g_jacob @ SIGMA[0:3, 0:3] @ g_jacob.T + Q[0:3, 0:3]\n",
    "\n",
    "        time_prev = time_curr\n",
    "        #current_txy = [event[1][0], ekf_state['x'][0], ekf_state['x'][1]]\n",
    "\n",
    "    elif event[0] == 'measurements':\n",
    "        #print('Measurements event:', event[1])\n",
    "        #print(i, 'th event, time:', time_curr)\n",
    "        # measurements of the currently seen landmarks from current scan: (range, angle, diameter)\n",
    "        x_list_curr = np.reshape(event[1][1:], [-1,3])\n",
    "\n",
    "        # find data association informations\n",
    "        # associations[i] ==  j if measurement i is associated with landmark j,\n",
    "        # associations[i] == -1 if measurement i is determined to be a new, previously unseen landmark, or,\n",
    "        # associations[i] == -2 if measurement i is too ambiguous to use and should be discarded.\n",
    "        associations = compute_data_association(MU, SIGMA,\n",
    "                                         num_of_seen_landmarks,\n",
    "                                         x_list_curr, R)\n",
    "\n",
    "        #print('Associations: ', associations)\n",
    "\n",
    "        # update the measurements x, update the landmark indices, and update the states of the newly seen landmarks\n",
    "        x = np.zeros((landmark_dim*num_of_total_landmarks,))\n",
    "        x_landmark_indices = []\n",
    "        #print(x_list_curr)\n",
    "        for x_curr, association in zip(x_list_curr, associations):\n",
    "          z_x, z_y, z_theta = MU[0:3]\n",
    "          d, phi, _ = x_curr # _, d, phi = x_curr\n",
    "          #print('Curr landm position:', x_curr)\n",
    "          #print('Distance to landmark:', d)\n",
    "          #print('angle of landmark:', phi)\n",
    "          if association == -2: # Outlier, ignore it\n",
    "            continue\n",
    "          elif association== -1: # new landmark\n",
    "            # update x\n",
    "            x_landmark_indices.append(num_of_seen_landmarks)\n",
    "            x[landmark_dim*num_of_seen_landmarks] = d\n",
    "            x[landmark_dim*num_of_seen_landmarks+1] = phi\n",
    "            # new landmark, init landmark state\n",
    "            m_x = z_x + d*np.cos(z_theta + phi)\n",
    "            m_y = z_y + d*np.sin(z_theta + phi)\n",
    "            MU[robot_dim + landmark_dim * num_of_seen_landmarks] += m_x\n",
    "            MU[robot_dim + landmark_dim * num_of_seen_landmarks + 1] += m_y\n",
    "            num_of_seen_landmarks += 1\n",
    "            #continue\n",
    "          else:\n",
    "            predicted_z = h_measurement(MU, association, np.zeros((2, 1)))\n",
    "            #print('Association:', association)\n",
    "            #print(\"Predicted:\", predicted_z.ravel(), \"Actual:\", x_curr[:2])\n",
    "            # update x\n",
    "            x_landmark_indices.append(association)\n",
    "            x[landmark_dim*association] = d\n",
    "            x[landmark_dim*association+1] = phi\n",
    "\n",
    "            #print('Robot state:', MU)\n",
    "            #print('Landmark number:', association)\n",
    "            h_measurement_out = np.zeros((2, 1))\n",
    "            h_measurement_out = h_measurement(MU, association, h_measurement_out)\n",
    "            h_jacob = h_jacobian(MU, association)\n",
    "\n",
    "            #print('SIGMA shape:', SIGMA.shape)\n",
    "            #print('R shape:', R.shape)\n",
    "            #print('h_jacob shape:', h_jacob.shape)\n",
    "            #print('h_measurement_out shape:', h_measurement_out.shape)\n",
    "\n",
    "            # innovation = np.array([[d], [phi]]) - h_measurement_out\n",
    "            # if np.linalg.norm(innovation) > 30:  # threshold depends on expected noise\n",
    "            #     print(f\"🚫 Skipping update due to large innovation: {innovation.ravel()}\")\n",
    "            #     continue\n",
    "\n",
    "            K = SIGMA @ h_jacob.T @ np.linalg.inv(h_jacob @ SIGMA @ h_jacob.T + R)\n",
    "            MU = MU + (K @ (np.array([[d], [phi]]) - h_measurement_out)).flatten()\n",
    "            SIGMA = SIGMA - K @ h_jacob @ SIGMA\n",
    "\n",
    "    elif event[0] == 'gps':\n",
    "        gps_history.append(event[1][:3])\n",
    "        continue\n",
    "\n",
    "    state_history['x'] = np.vstack((state_history['x'], MU[0:3]))\n",
    "    #print('State history:', state_history['x'][-1])\n",
    "    state_history['P'] = np.vstack((state_history['P'], np.diag(SIGMA[:3,:3])))\n",
    "    state_history['robot_pose_history'].append([time_prev, MU[0], MU[1]])\n",
    "\n",
    "# end of data\n",
    "ATE_RMSE, idxs_gps, idxs_est = cal_ate_rmse(state_history['robot_pose_history'], gps_history)\n",
    "\n",
    "# plot results\n",
    "plt.cla()\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "## GPS\n",
    "gps_history = np.array(gps_history)\n",
    "plt.plot(gps_history[:,1], gps_history[:,2], '^', markersize=2, color='tab:red', label='gps')\n",
    "## Estimated Traj\n",
    "plt.plot(state_history['x'][:,0], state_history['x'][:,1], color='tab:blue', linewidth=1, label='est traj')\n",
    "## Landmarks\n",
    "trees_plot = np.array(MU[3:]).reshape(-1,2)\n",
    "plt.plot(trees_plot[:,0], trees_plot[:,1], 'xg', markersize=4, label='landmarks')\n",
    "\n",
    "## Optional, to check associations between est traj and gps datas\n",
    "for i in range(len(idxs_gps)):\n",
    "    plt.plot(   [state_history['x'][idxs_est[i],0], gps_history[idxs_gps[i],1]],\\\n",
    "                [state_history['x'][idxs_est[i],1], gps_history[idxs_gps[i],2]],\\\n",
    "                color='tab:orange', linewidth=0.3)\n",
    "\n",
    "print(\"num_trees\", num_of_seen_landmarks)\n",
    "\n",
    "plt.title(f\"ATE_RMSE: {ATE_RMSE:.3f}[m]\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dde3XCbAb1sv"
   },
   "outputs": [],
   "source": [
    "events, robot_init_pose = np.load('data.npy', allow_pickle=True)\n",
    "\n",
    "'''\n",
    "  it takes about 10 minuts for all trajectory estimation.\n",
    "  for debuging you can crop the events as the following:\n",
    "'''\n",
    "print(robot_init_pose)\n",
    "events = events[:10] #\n",
    "for e in events:\n",
    "  if e[0] == 'measurements':\n",
    "    e_np = np.array(e[1][1:])\n",
    "    print(e_np.shape)\n",
    "    print(e_np[0][0])\n",
    "    #print(len(e[1][1:][0]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nasa_cllg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
